{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "import os\n",
    "import vtk\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "import numpy as np\n",
    "from vtk.util.numpy_support import numpy_to_vtk\n",
    "\n",
    "class OFFReader():\n",
    "\tdef __init__(self):\n",
    "\t\tFileName = None\n",
    "\t\tOutput = None\n",
    "\n",
    "\tdef SetFileName(self, fileName):\n",
    "\t\tself.FileName = fileName\n",
    "\n",
    "\tdef GetOutput(self):\n",
    "\t\treturn self.Output\n",
    "\n",
    "\tdef Update(self):\n",
    "\t\twith open(self.FileName) as file:\n",
    "\n",
    "\t\t\tfirst_string = file.readline() # either 'OFF' or 'OFFxxxx xxxx x'\n",
    "\n",
    "\t\t\tif 'OFF' != first_string[0:3]:\n",
    "\t\t\t\traise('Not a valid OFF header!')\n",
    "\n",
    "\t\t\telif first_string[3:4] != '\\n':\n",
    "\t\t\t\tnew_first = 'OFF'\n",
    "\t\t\t\tnew_second = first_string[3:]\n",
    "\t\t\t\tn_verts, n_faces, n_dontknow = tuple([int(s) for s in new_second.strip().split(' ')])\t\t\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tn_verts, n_faces, n_dontknow = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "\n",
    "\t\t\tsurf = vtk.vtkPolyData()\n",
    "\t\t\tpoints = vtk.vtkPoints()\n",
    "\t\t\tcells = vtk.vtkCellArray()\n",
    "\n",
    "\t\t\tfor i_vert in range(n_verts):\n",
    "\t\t\t\tp = [float(s) for s in file.readline().strip().split(' ')]\n",
    "\t\t\t\tpoints.InsertNextPoint(p[0], p[1], p[2])\n",
    "\n",
    "\t\t\tfor i_face in range(n_faces):\n",
    "\t\t\t\t\n",
    "\t\t\t\tt = [int(s) for s in file.readline().strip().split(' ')]\n",
    "\n",
    "\t\t\t\tif(t[0] == 1):\n",
    "\t\t\t\t\tvertex = vtk.vtkVertex()\n",
    "\t\t\t\t\tvertex.GetPointIds().SetId(0, t[1])\n",
    "\t\t\t\t\tcells.InsertNextCell(line)\n",
    "\t\t\t\telif(t[0] == 2):\n",
    "\t\t\t\t\tline = vtk.vtkLine()\n",
    "\t\t\t\t\tline.GetPointIds().SetId(0, t[1])\n",
    "\t\t\t\t\tline.GetPointIds().SetId(1, t[2])\n",
    "\t\t\t\t\tcells.InsertNextCell(line)\n",
    "\t\t\t\telif(t[0] == 3):\n",
    "\t\t\t\t\ttriangle = vtk.vtkTriangle()\n",
    "\t\t\t\t\ttriangle.GetPointIds().SetId(0, t[1])\n",
    "\t\t\t\t\ttriangle.GetPointIds().SetId(1, t[2])\n",
    "\t\t\t\t\ttriangle.GetPointIds().SetId(2, t[3])\n",
    "\t\t\t\t\tcells.InsertNextCell(triangle)\n",
    "\n",
    "\t\t\tsurf.SetPoints(points)\n",
    "\t\t\tsurf.SetPolys(cells)\n",
    "\n",
    "\t\t\tself.Output = surf\n",
    "\n",
    "def ScaleSurf(surf, mean_arr = None, scale_factor = None, copy=True):\n",
    "    if(copy):\n",
    "        surf_copy = vtk.vtkPolyData()\n",
    "        surf_copy.DeepCopy(surf)\n",
    "        surf = surf_copy\n",
    "\n",
    "    shapedatapoints = surf.GetPoints()\n",
    "\n",
    "    #calculate bounding box\n",
    "    mean_v = [0.0] * 3\n",
    "    bounds_max_v = [0.0] * 3\n",
    "\n",
    "    bounds = shapedatapoints.GetBounds()\n",
    "\n",
    "    mean_v[0] = (bounds[0] + bounds[1])/2.0\n",
    "    mean_v[1] = (bounds[2] + bounds[3])/2.0\n",
    "    mean_v[2] = (bounds[4] + bounds[5])/2.0\n",
    "    bounds_max_v[0] = max(bounds[0], bounds[1])\n",
    "    bounds_max_v[1] = max(bounds[2], bounds[3])\n",
    "    bounds_max_v[2] = max(bounds[4], bounds[5])\n",
    "\n",
    "    shape_points = vtk_to_numpy(shapedatapoints.GetData())\n",
    "    \n",
    "    #centering points of the shape\n",
    "    if mean_arr is None:\n",
    "        mean_arr = np.array(mean_v)\n",
    "    # print(\"Mean:\", mean_arr)\n",
    "    shape_points = shape_points - mean_arr\n",
    "\n",
    "    #Computing scale factor if it is not provided\n",
    "    if(scale_factor is None):\n",
    "        bounds_max_arr = np.array(bounds_max_v)\n",
    "        scale_factor = 1.0/np.linalg.norm(bounds_max_arr - mean_arr)\n",
    "\n",
    "    #scale points of the shape by scale factor\n",
    "    # print(\"Scale:\", scale_factor)\n",
    "    shape_points = np.multiply(shape_points, scale_factor)\n",
    "\n",
    "    #assigning scaled points back to shape\n",
    "    shapedatapoints.SetData(numpy_to_vtk(shape_points))\n",
    "\n",
    "    return surf, mean_arr, scale_factor\n",
    "\n",
    "def ScaleSurfT(surf, mean_arr=None, scale_factor=None, copy=True):\n",
    "    if copy:\n",
    "        # Perform a deep copy if needed (create a new tensor with the same data)\n",
    "        surf = surf.clone()\n",
    "\n",
    "    if mean_arr is None:\n",
    "        mean_arr = surf.mean(dim=0)\n",
    "    \n",
    "    bounds_max_arr = surf.max(dim=0)[0]\n",
    "\n",
    "    # Centering points of the shape\n",
    "    surf = surf - mean_arr\n",
    "\n",
    "    # Computing scale factor if it is not provided\n",
    "    if scale_factor is None:\n",
    "        scale_factor = 1.0 / (bounds_max_arr - mean_arr).norm()\n",
    "\n",
    "    # Scale points of the shape by scale factor\n",
    "    surf = surf * scale_factor\n",
    "\n",
    "    return surf, mean_arr, scale_factor\n",
    "\n",
    "class UnitSurfTransform:\n",
    "    # This transform is used to make sure that the surface is in the unit cube\n",
    "    def __init__(self, scale_factor=None):\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __call__(self, surf):\n",
    "        if isinstance(surf, torch.Tensor):\n",
    "            return GetUnitSurfT(surf)\n",
    "        else:\n",
    "            return GetUnitSurf(surf)\n",
    "\n",
    "def GetUnitSurf(surf, mean_arr = None, scale_factor = None, copy=True):\n",
    "    unit_surf, surf_mean, surf_scale = ScaleSurf(surf, mean_arr, scale_factor, copy)\n",
    "    return unit_surf\n",
    "\n",
    "def GetUnitSurfT(surf, mean_arr=None, scale_factor=None, copy=True):\n",
    "    unit_surf, surf_mean, surf_scale = ScaleSurfT(surf, mean_arr, scale_factor, copy)\n",
    "    return unit_surf\n",
    "\n",
    "def data_to_tensor(path):\n",
    "    data = nib.freesurfer.read_morph_data(path)\n",
    "    data = data.byteswap().newbyteorder()\n",
    "    data = torch.from_numpy(data).float()\n",
    "    return data\n",
    "\n",
    "def ReadSurf(fileName):\n",
    "\n",
    "    fname, extension = os.path.splitext(fileName)    \n",
    "    extension = extension.lower()    \n",
    "    if extension == \".vtk\":\n",
    "        reader = vtk.vtkPolyDataReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()\n",
    "    elif extension == \".vtp\":\n",
    "        reader = vtk.vtkXMLPolyDataReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()    \n",
    "    elif extension == \".stl\":\n",
    "        reader = vtk.vtkSTLReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()\n",
    "    elif extension == \".off\":\n",
    "        reader = OFFReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()\n",
    "    elif extension == \".obj\":\n",
    "        if os.path.exists(fname + \".mtl\"):\n",
    "            obj_import = vtk.vtkOBJImporter()\n",
    "            obj_import.SetFileName(fileName)\n",
    "            obj_import.SetFileNameMTL(fname + \".mtl\")\n",
    "            textures_path = os.path.normpath(os.path.dirname(fname) + \"/../images\")\n",
    "            if os.path.exists(textures_path):\n",
    "                textures_path = os.path.normpath(fname.replace(os.path.basename(fname), ''))\n",
    "                obj_import.SetTexturePath(textures_path)\n",
    "            else:\n",
    "                textures_path = os.path.normpath(fname.replace(os.path.basename(fname), ''))                \n",
    "                obj_import.SetTexturePath(textures_path)\n",
    "                    \n",
    "\n",
    "            obj_import.Read()\n",
    "\n",
    "            actors = obj_import.GetRenderer().GetActors()\n",
    "            actors.InitTraversal()\n",
    "            append = vtk.vtkAppendPolyData()\n",
    "\n",
    "            for i in range(actors.GetNumberOfItems()):\n",
    "                surfActor = actors.GetNextActor()\n",
    "                append.AddInputData(surfActor.GetMapper().GetInputAsDataSet())\n",
    "            \n",
    "            append.Update()\n",
    "            surf = append.GetOutput()\n",
    "            \n",
    "        else:\n",
    "            reader = vtk.vtkOBJReader()\n",
    "            reader.SetFileName(fileName)\n",
    "            reader.Update()\n",
    "            surf = reader.GetOutput()\n",
    "    elif extension == '.gii':\n",
    "\n",
    "        import nibabel as nib\n",
    "        from fsl.data import gifti\n",
    "\n",
    "        surf = nib.load(fileName)\n",
    "        coords = surf.agg_data('pointset')\n",
    "        triangles = surf.agg_data('triangle')\n",
    "\n",
    "        points = vtk.vtkPoints()\n",
    "\n",
    "        for c in coords:\n",
    "            points.InsertNextPoint(c[0], c[1], c[2])\n",
    "\n",
    "        cells = vtk.vtkCellArray()\n",
    "\n",
    "        for t in triangles:\n",
    "            t_vtk = vtk.vtkTriangle()\n",
    "            t_vtk.GetPointIds().SetId(0, t[0])\n",
    "            t_vtk.GetPointIds().SetId(1, t[1])\n",
    "            t_vtk.GetPointIds().SetId(2, t[2])\n",
    "            cells.InsertNextCell(t_vtk)\n",
    "\n",
    "        surf = vtk.vtkPolyData()\n",
    "        surf.SetPoints(points)\n",
    "        surf.SetPolys(cells)\n",
    "    else:\n",
    "        raise Exception(\"File format not supported\")\n",
    "    \n",
    "    return surf\n",
    "\n",
    "def PolyDataToTensors_v_f(surf, device='cpu'):\n",
    "\n",
    "    verts, faces, = PolyDataToNumpy_v_f(surf)\n",
    "    \n",
    "    verts = torch.tensor(verts).to(torch.float32).to(device)\n",
    "    faces = torch.tensor(faces).to(torch.int64).to(device)\n",
    "    \n",
    "    return verts, faces\n",
    "\n",
    "def PolyDataToNumpy_v_f(surf):\n",
    "\n",
    "    vtk.vtkObject.GlobalWarningDisplayOff()\n",
    "    verts = vtk_to_numpy(surf.GetPoints().GetData())\n",
    "    faces = vtk_to_numpy(surf.GetPolys().GetData()).reshape(-1, 4)[:,1:]\n",
    "    \n",
    "    return verts, faces\n",
    "\n",
    "def compute_verts(path):\n",
    "    wm_vtk_path = os.path.join(path, f'lh.white.vtk')\n",
    "    surf = ReadSurf(wm_vtk_path)\n",
    "\n",
    "    transform = UnitSurfTransform()\n",
    "    surf_norm = transform(surf)\n",
    "\n",
    "    verts, faces = PolyDataToTensors_v_f(surf_norm)\n",
    "    \n",
    "    return verts\n",
    "\n",
    "path1 = '/CMF/data/floda/abcd-data-release-5.1/data/sub-NDARINV021N0FLH/sub-NDARINV021N0FLH_ses-baselineYear1Arm1/surf/'\n",
    "path2 = '/CMF/data/floda/abcd-data-release-5.1/data/sub-NDARINV028D3ELL/sub-NDARINV028D3ELL_ses-4YearFollowUpYArm1/surf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ocnn\n",
    "from ocnn.octree import Octree, Points\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class PatchEmbed(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int = 3, dim: int = 96, num_down: int = 2,\n",
    "               nempty: bool = True, **kwargs):\n",
    "    super().__init__()\n",
    "    self.num_stages = num_down\n",
    "    self.delta_depth = -num_down\n",
    "    channels = [int(dim * 2**i) for i in range(-self.num_stages, 1)]\n",
    "\n",
    "    self.convs = torch.nn.ModuleList([ocnn.modules.OctreeConvBnRelu(\n",
    "        in_channels if i == 0 else channels[i], channels[i], kernel_size=[3],\n",
    "        stride=1, nempty=nempty) for i in range(self.num_stages)])\n",
    "    self.downsamples = torch.nn.ModuleList([ocnn.modules.OctreeConvBnRelu(\n",
    "        channels[i], channels[i+1], kernel_size=[2], stride=2, nempty=nempty)\n",
    "        for i in range(self.num_stages)])\n",
    "    self.proj = ocnn.modules.OctreeConvBnRelu(\n",
    "        channels[-1], dim, kernel_size=[3], stride=1, nempty=nempty)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "      print(\"Data shape:\", data.shape)\n",
    "      for i in range(self.num_stages):\n",
    "        depth_i = depth - i\n",
    "        print(\"Depth:\", depth_i)\n",
    "        data = self.convs[i](data, octree, depth_i)\n",
    "        print(\"coucou\")\n",
    "        data = self.downsamples[i](data, octree, depth_i)\n",
    "      data = self.proj(data, octree, depth_i - 1)\n",
    "      return data\n",
    "\n",
    "class OctFormer(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int,\n",
    "               channels: List[int] = [96, 192, 384, 384],\n",
    "               num_blocks: List[int] = [2, 2, 18, 2],\n",
    "               num_heads: List[int] = [6, 12, 24, 24],\n",
    "               patch_size: int = 26, dilation: int = 4, drop_path: float = 0.5,\n",
    "               nempty: bool = True, stem_down: int = 2, **kwargs):\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.dilation = dilation\n",
    "    self.nempty = nempty\n",
    "    self.num_stages = len(num_blocks)\n",
    "    self.stem_down = stem_down\n",
    "    drop_ratio = torch.linspace(0, drop_path, sum(num_blocks)).tolist()\n",
    "\n",
    "    self.patch_embed = PatchEmbed(in_channels, channels[0], stem_down, nempty)\n",
    "    # self.layers = torch.nn.ModuleList([OctFormerStage(\n",
    "    #     dim=channels[i], num_heads=num_heads[i], patch_size=patch_size,\n",
    "    #     drop_path=drop_ratio[sum(num_blocks[:i]):sum(num_blocks[:i+1])],\n",
    "    #     dilation=dilation, nempty=nempty, num_blocks=num_blocks[i],)\n",
    "    #     for i in range(self.num_stages)])\n",
    "    # self.downsamples = torch.nn.ModuleList([Downsample(\n",
    "    #     channels[i], channels[i + 1], kernel_size=[2],\n",
    "    #     nempty=nempty) for i in range(self.num_stages - 1)])\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    data = self.patch_embed(data, octree, depth)\n",
    "    depth = depth - self.stem_down   # current octree depth\n",
    "    octree = OctreeT(octree, self.patch_size, self.dilation, self.nempty,\n",
    "                     max_depth=depth, start_depth=depth-self.num_stages+1)\n",
    "    features = {}\n",
    "    for i in range(self.num_stages):\n",
    "      depth_i = depth - i\n",
    "      data = self.layers[i](data, octree, depth_i)\n",
    "      features[depth_i] = data\n",
    "      if i < self.num_stages - 1:\n",
    "        data = self.downsamples[i](data, octree, depth_i)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ClsHeader(torch.nn.Module):\n",
    "  def __init__(self, out_channels: int, in_channels: int,\n",
    "               nempty: bool = False, dropout: float = 0.5):\n",
    "    super().__init__()\n",
    "    self.global_pool = ocnn.nn.OctreeGlobalPool(nempty)\n",
    "    self.cls_header = torch.nn.Sequential(\n",
    "        ocnn.modules.FcBnRelu(in_channels, 256),\n",
    "        torch.nn.Dropout(p=dropout),\n",
    "        torch.nn.Linear(256, out_channels))\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    data = self.global_pool(data, octree, depth)\n",
    "    logit = self.cls_header(data)\n",
    "    return logit\n",
    "\n",
    "\n",
    "class OctFormerCls(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int, out_channels: int,\n",
    "               channels: List[int] = [96, 192, 384, 384],\n",
    "               num_blocks: List[int] = [2, 2, 18, 2],\n",
    "               num_heads: List[int] = [6, 12, 24, 24],\n",
    "               patch_size: int = 32, dilation: int = 4,\n",
    "               drop_path: float = 0.5, nempty: bool = True,\n",
    "               stem_down: int = 2, head_drop: float = 0.5, **kwargs):\n",
    "    super().__init__()\n",
    "    self.backbone = OctFormer(\n",
    "        in_channels, channels, num_blocks, num_heads, patch_size, dilation,\n",
    "        drop_path, nempty, stem_down)\n",
    "    # self.head = ClsHeader(\n",
    "    #     out_channels, channels[-1], nempty, head_drop)\n",
    "    # self.apply(self.init_weights)\n",
    "\n",
    "  def init_weights(self, m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "      torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "      if isinstance(m, torch.nn.Linear) and m.bias is not None:\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    features = self.backbone(data, octree, depth)\n",
    "    curr_depth = min(features.keys())\n",
    "    output = self.head(features[curr_depth], octree, curr_depth)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUnitSurf(surf, mean_arr = None, scale_factor = None, copy=True):\n",
    "    unit_surf, surf_mean, surf_scale = ScaleSurf(surf, mean_arr, scale_factor, copy)\n",
    "    return unit_surf\n",
    "\n",
    "def GetNormalsTensor(surf):\n",
    "    normals = ComputeNormals(surf)\n",
    "    normals = vtk_to_numpy(normals.GetPointData().GetNormals())\n",
    "    return torch.tensor(normals, dtype=torch.double)\n",
    "\n",
    "def ComputeNormals(surf):\n",
    "    normals = vtk.vtkPolyDataNormals()\n",
    "    normals.SetInputData(surf);\n",
    "    normals.ComputeCellNormalsOn();\n",
    "    normals.ComputePointNormalsOn();\n",
    "    normals.SplittingOff();\n",
    "    normals.Update()\n",
    "    \n",
    "    return normals.GetOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([4464, 4])\n",
      "Depth: 6\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The shape of input data is wrong.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 47\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moctree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moctree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 118\u001b[0m, in \u001b[0;36mOctFormerCls.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, octree: Octree, depth: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m   features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m   curr_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(features\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    120\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(features[curr_depth], octree, curr_depth)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 62\u001b[0m, in \u001b[0;36mOctFormer.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, octree: Octree, depth: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m   depth \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem_down   \u001b[38;5;66;03m# current octree depth\u001b[39;00m\n\u001b[1;32m     64\u001b[0m   octree \u001b[38;5;241m=\u001b[39m OctreeT(octree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnempty,\n\u001b[1;32m     65\u001b[0m                    max_depth\u001b[38;5;241m=\u001b[39mdepth, start_depth\u001b[38;5;241m=\u001b[39mdepth\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stages\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m     27\u001b[0m depth_i \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m-\u001b[39m i\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepth:\u001b[39m\u001b[38;5;124m\"\u001b[39m, depth_i)\n\u001b[0;32m---> 29\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoucou\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamples[i](data, octree, depth_i)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/modules/modules.py:71\u001b[0m, in \u001b[0;36mOctreeConvBnRelu.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, octree: Octree, depth: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m''''''\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(out)\n\u001b[1;32m     73\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/nn/octree_conv.py:357\u001b[0m, in \u001b[0;36mOctreeConv.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m    355\u001b[0m   out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(col\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43moctree_conv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnempty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    363\u001b[0m   out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/nn/octree_conv.py:224\u001b[0m, in \u001b[0;36mOctreeConvFunction.forward\u001b[0;34m(ctx, data, weights, octree, depth, in_channels, out_channels, kernel_size, stride, nempty, max_buffer)\u001b[0m\n\u001b[1;32m    221\u001b[0m octree_conv \u001b[38;5;241m=\u001b[39m _OctreeConv(\n\u001b[1;32m    222\u001b[0m     in_channels, out_channels, kernel_size, stride, nempty, max_buffer)\n\u001b[1;32m    223\u001b[0m octree_conv\u001b[38;5;241m.\u001b[39msetup(octree, depth)\n\u001b[0;32m--> 224\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43moctree_conv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m out \u001b[38;5;241m=\u001b[39m octree_conv\u001b[38;5;241m.\u001b[39mforward_gemm(out, data, weights)\n\u001b[1;32m    227\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(data, weights)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/nn/octree_conv.py:101\u001b[0m, in \u001b[0;36mOctreeConvBase.check_and_init\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Check the shape of input data\u001b[39;00m\n\u001b[1;32m    100\u001b[0m check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_shape\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m check, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe shape of input data is wrong.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Init the output data\u001b[39;00m\n\u001b[1;32m    104\u001b[0m out \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnew_zeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_shape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The shape of input data is wrong."
     ]
    }
   ],
   "source": [
    "from ocnn.dataset import CollateBatch\n",
    "from thsolver import Dataset\n",
    "\n",
    "# channel: 4\n",
    "# feature: ND\n",
    "# find_unused_parameters: False\n",
    "# name: octformercls\n",
    "# nempty: False\n",
    "# nout: 40\n",
    "# sync_bn: False\n",
    "# use_checkpoint: False\n",
    "\n",
    "# data = octree.get_input_feature('ND').to(torch.float)\n",
    "# resnet = OctFormerCls(in_channels=4, out_channels=40) \n",
    "# resnet(data, octree=octree, depth=6).shape\n",
    "\n",
    "path = '/work/floda/source/tools/octformer/data/ModelNet40/ModelNet40/airplane/train/airplane_0001.off'\n",
    "\n",
    "surf1 = ReadSurf(path)\n",
    "surf1 = GetUnitSurf(surf1)\n",
    "V1, F1 = PolyDataToTensors_v_f(surf1)\n",
    "N1 = GetNormalsTensor(surf1)\n",
    "\n",
    "octree = Octree(6)\n",
    "octree.build_octree(Points(V1, normals=N1))\n",
    "octree.construct_all_neigh()\n",
    "octree_feature = ocnn.modules.InputFeature('ND')\n",
    "data = octree_feature(octree)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "in_channels = 4 \n",
    "out_channels = 1\n",
    "\n",
    "model = OctFormerCls(in_channels=in_channels, out_channels=out_channels)\n",
    "model = model.to(device)\n",
    "\n",
    "label = torch.tensor([0]).to(device) \n",
    "fake_batch = {\n",
    "    'octree': octree.to(device),\n",
    "    'data': data.to(device),\n",
    "    'label': label\n",
    "}\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(fake_batch['data'], fake_batch['octree'], fake_batch['octree'].depth)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
