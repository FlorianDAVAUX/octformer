{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import torch\n",
    "import os\n",
    "import vtk\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "import numpy as np\n",
    "from vtk.util.numpy_support import numpy_to_vtk\n",
    "\n",
    "class OFFReader():\n",
    "\tdef __init__(self):\n",
    "\t\tFileName = None\n",
    "\t\tOutput = None\n",
    "\n",
    "\tdef SetFileName(self, fileName):\n",
    "\t\tself.FileName = fileName\n",
    "\n",
    "\tdef GetOutput(self):\n",
    "\t\treturn self.Output\n",
    "\n",
    "\tdef Update(self):\n",
    "\t\twith open(self.FileName) as file:\n",
    "\n",
    "\t\t\tfirst_string = file.readline() # either 'OFF' or 'OFFxxxx xxxx x'\n",
    "\n",
    "\t\t\tif 'OFF' != first_string[0:3]:\n",
    "\t\t\t\traise('Not a valid OFF header!')\n",
    "\n",
    "\t\t\telif first_string[3:4] != '\\n':\n",
    "\t\t\t\tnew_first = 'OFF'\n",
    "\t\t\t\tnew_second = first_string[3:]\n",
    "\t\t\t\tn_verts, n_faces, n_dontknow = tuple([int(s) for s in new_second.strip().split(' ')])\t\t\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tn_verts, n_faces, n_dontknow = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "\n",
    "\t\t\tsurf = vtk.vtkPolyData()\n",
    "\t\t\tpoints = vtk.vtkPoints()\n",
    "\t\t\tcells = vtk.vtkCellArray()\n",
    "\n",
    "\t\t\tfor i_vert in range(n_verts):\n",
    "\t\t\t\tp = [float(s) for s in file.readline().strip().split(' ')]\n",
    "\t\t\t\tpoints.InsertNextPoint(p[0], p[1], p[2])\n",
    "\n",
    "\t\t\tfor i_face in range(n_faces):\n",
    "\t\t\t\t\n",
    "\t\t\t\tt = [int(s) for s in file.readline().strip().split(' ')]\n",
    "\n",
    "\t\t\t\tif(t[0] == 1):\n",
    "\t\t\t\t\tvertex = vtk.vtkVertex()\n",
    "\t\t\t\t\tvertex.GetPointIds().SetId(0, t[1])\n",
    "\t\t\t\t\tcells.InsertNextCell(line)\n",
    "\t\t\t\telif(t[0] == 2):\n",
    "\t\t\t\t\tline = vtk.vtkLine()\n",
    "\t\t\t\t\tline.GetPointIds().SetId(0, t[1])\n",
    "\t\t\t\t\tline.GetPointIds().SetId(1, t[2])\n",
    "\t\t\t\t\tcells.InsertNextCell(line)\n",
    "\t\t\t\telif(t[0] == 3):\n",
    "\t\t\t\t\ttriangle = vtk.vtkTriangle()\n",
    "\t\t\t\t\ttriangle.GetPointIds().SetId(0, t[1])\n",
    "\t\t\t\t\ttriangle.GetPointIds().SetId(1, t[2])\n",
    "\t\t\t\t\ttriangle.GetPointIds().SetId(2, t[3])\n",
    "\t\t\t\t\tcells.InsertNextCell(triangle)\n",
    "\n",
    "\t\t\tsurf.SetPoints(points)\n",
    "\t\t\tsurf.SetPolys(cells)\n",
    "\n",
    "\t\t\tself.Output = surf\n",
    "\n",
    "def ScaleSurf(surf, mean_arr = None, scale_factor = None, copy=True):\n",
    "    if(copy):\n",
    "        surf_copy = vtk.vtkPolyData()\n",
    "        surf_copy.DeepCopy(surf)\n",
    "        surf = surf_copy\n",
    "\n",
    "    shapedatapoints = surf.GetPoints()\n",
    "\n",
    "    #calculate bounding box\n",
    "    mean_v = [0.0] * 3\n",
    "    bounds_max_v = [0.0] * 3\n",
    "\n",
    "    bounds = shapedatapoints.GetBounds()\n",
    "\n",
    "    mean_v[0] = (bounds[0] + bounds[1])/2.0\n",
    "    mean_v[1] = (bounds[2] + bounds[3])/2.0\n",
    "    mean_v[2] = (bounds[4] + bounds[5])/2.0\n",
    "    bounds_max_v[0] = max(bounds[0], bounds[1])\n",
    "    bounds_max_v[1] = max(bounds[2], bounds[3])\n",
    "    bounds_max_v[2] = max(bounds[4], bounds[5])\n",
    "\n",
    "    shape_points = vtk_to_numpy(shapedatapoints.GetData())\n",
    "    \n",
    "    #centering points of the shape\n",
    "    if mean_arr is None:\n",
    "        mean_arr = np.array(mean_v)\n",
    "    # print(\"Mean:\", mean_arr)\n",
    "    shape_points = shape_points - mean_arr\n",
    "\n",
    "    #Computing scale factor if it is not provided\n",
    "    if(scale_factor is None):\n",
    "        bounds_max_arr = np.array(bounds_max_v)\n",
    "        scale_factor = 1.0/np.linalg.norm(bounds_max_arr - mean_arr)\n",
    "\n",
    "    #scale points of the shape by scale factor\n",
    "    # print(\"Scale:\", scale_factor)\n",
    "    shape_points = np.multiply(shape_points, scale_factor)\n",
    "\n",
    "    #assigning scaled points back to shape\n",
    "    shapedatapoints.SetData(numpy_to_vtk(shape_points))\n",
    "\n",
    "    return surf, mean_arr, scale_factor\n",
    "\n",
    "def ScaleSurfT(surf, mean_arr=None, scale_factor=None, copy=True):\n",
    "    if copy:\n",
    "        # Perform a deep copy if needed (create a new tensor with the same data)\n",
    "        surf = surf.clone()\n",
    "\n",
    "    if mean_arr is None:\n",
    "        mean_arr = surf.mean(dim=0)\n",
    "    \n",
    "    bounds_max_arr = surf.max(dim=0)[0]\n",
    "\n",
    "    # Centering points of the shape\n",
    "    surf = surf - mean_arr\n",
    "\n",
    "    # Computing scale factor if it is not provided\n",
    "    if scale_factor is None:\n",
    "        scale_factor = 1.0 / (bounds_max_arr - mean_arr).norm()\n",
    "\n",
    "    # Scale points of the shape by scale factor\n",
    "    surf = surf * scale_factor\n",
    "\n",
    "    return surf, mean_arr, scale_factor\n",
    "\n",
    "class UnitSurfTransform:\n",
    "    # This transform is used to make sure that the surface is in the unit cube\n",
    "    def __init__(self, scale_factor=None):\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "    def __call__(self, surf):\n",
    "        if isinstance(surf, torch.Tensor):\n",
    "            return GetUnitSurfT(surf)\n",
    "        else:\n",
    "            return GetUnitSurf(surf)\n",
    "\n",
    "def GetUnitSurf(surf, mean_arr = None, scale_factor = None, copy=True):\n",
    "    unit_surf, surf_mean, surf_scale = ScaleSurf(surf, mean_arr, scale_factor, copy)\n",
    "    return unit_surf\n",
    "\n",
    "def GetUnitSurfT(surf, mean_arr=None, scale_factor=None, copy=True):\n",
    "    unit_surf, surf_mean, surf_scale = ScaleSurfT(surf, mean_arr, scale_factor, copy)\n",
    "    return unit_surf\n",
    "\n",
    "def data_to_tensor(path):\n",
    "    data = nib.freesurfer.read_morph_data(path)\n",
    "    data = data.byteswap().newbyteorder()\n",
    "    data = torch.from_numpy(data).float()\n",
    "    return data\n",
    "\n",
    "def ReadSurf(fileName):\n",
    "\n",
    "    fname, extension = os.path.splitext(fileName)    \n",
    "    extension = extension.lower()    \n",
    "    if extension == \".vtk\":\n",
    "        reader = vtk.vtkPolyDataReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()\n",
    "    elif extension == \".vtp\":\n",
    "        reader = vtk.vtkXMLPolyDataReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()    \n",
    "    elif extension == \".stl\":\n",
    "        reader = vtk.vtkSTLReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()\n",
    "    elif extension == \".off\":\n",
    "        reader = OFFReader()\n",
    "        reader.SetFileName(fileName)\n",
    "        reader.Update()\n",
    "        surf = reader.GetOutput()\n",
    "    elif extension == \".obj\":\n",
    "        if os.path.exists(fname + \".mtl\"):\n",
    "            obj_import = vtk.vtkOBJImporter()\n",
    "            obj_import.SetFileName(fileName)\n",
    "            obj_import.SetFileNameMTL(fname + \".mtl\")\n",
    "            textures_path = os.path.normpath(os.path.dirname(fname) + \"/../images\")\n",
    "            if os.path.exists(textures_path):\n",
    "                textures_path = os.path.normpath(fname.replace(os.path.basename(fname), ''))\n",
    "                obj_import.SetTexturePath(textures_path)\n",
    "            else:\n",
    "                textures_path = os.path.normpath(fname.replace(os.path.basename(fname), ''))                \n",
    "                obj_import.SetTexturePath(textures_path)\n",
    "                    \n",
    "\n",
    "            obj_import.Read()\n",
    "\n",
    "            actors = obj_import.GetRenderer().GetActors()\n",
    "            actors.InitTraversal()\n",
    "            append = vtk.vtkAppendPolyData()\n",
    "\n",
    "            for i in range(actors.GetNumberOfItems()):\n",
    "                surfActor = actors.GetNextActor()\n",
    "                append.AddInputData(surfActor.GetMapper().GetInputAsDataSet())\n",
    "            \n",
    "            append.Update()\n",
    "            surf = append.GetOutput()\n",
    "            \n",
    "        else:\n",
    "            reader = vtk.vtkOBJReader()\n",
    "            reader.SetFileName(fileName)\n",
    "            reader.Update()\n",
    "            surf = reader.GetOutput()\n",
    "    elif extension == '.gii':\n",
    "\n",
    "        import nibabel as nib\n",
    "        from fsl.data import gifti\n",
    "\n",
    "        surf = nib.load(fileName)\n",
    "        coords = surf.agg_data('pointset')\n",
    "        triangles = surf.agg_data('triangle')\n",
    "\n",
    "        points = vtk.vtkPoints()\n",
    "\n",
    "        for c in coords:\n",
    "            points.InsertNextPoint(c[0], c[1], c[2])\n",
    "\n",
    "        cells = vtk.vtkCellArray()\n",
    "\n",
    "        for t in triangles:\n",
    "            t_vtk = vtk.vtkTriangle()\n",
    "            t_vtk.GetPointIds().SetId(0, t[0])\n",
    "            t_vtk.GetPointIds().SetId(1, t[1])\n",
    "            t_vtk.GetPointIds().SetId(2, t[2])\n",
    "            cells.InsertNextCell(t_vtk)\n",
    "\n",
    "        surf = vtk.vtkPolyData()\n",
    "        surf.SetPoints(points)\n",
    "        surf.SetPolys(cells)\n",
    "    else:\n",
    "        raise Exception(\"File format not supported\")\n",
    "    \n",
    "    return surf\n",
    "\n",
    "def PolyDataToTensors_v_f(surf, device='cpu'):\n",
    "\n",
    "    verts, faces, = PolyDataToNumpy_v_f(surf)\n",
    "    \n",
    "    verts = torch.tensor(verts).to(torch.float32).to(device)\n",
    "    faces = torch.tensor(faces).to(torch.int64).to(device)\n",
    "    \n",
    "    return verts, faces\n",
    "\n",
    "def PolyDataToNumpy_v_f(surf):\n",
    "\n",
    "    vtk.vtkObject.GlobalWarningDisplayOff()\n",
    "    verts = vtk_to_numpy(surf.GetPoints().GetData())\n",
    "    faces = vtk_to_numpy(surf.GetPolys().GetData()).reshape(-1, 4)[:,1:]\n",
    "    \n",
    "    return verts, faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ocnn\n",
    "\n",
    "from ocnn.octree import Octree, Points\n",
    "from typing import Optional, List\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "\n",
    "class OctreeT(Octree):\n",
    "\n",
    "  def __init__(self, octree: Octree, patch_size: int = 24, dilation: int = 4,\n",
    "               nempty: bool = True, max_depth: Optional[int] = None,\n",
    "               start_depth: Optional[int] = None, **kwargs):\n",
    "    super().__init__(octree.depth, octree.full_depth)\n",
    "    self.__dict__.update(octree.__dict__)\n",
    "\n",
    "    self.patch_size = patch_size\n",
    "    self.dilation = dilation  # TODO dilation as a list\n",
    "    self.nempty = nempty\n",
    "    self.max_depth = max_depth or self.depth\n",
    "    self.start_depth = start_depth or self.full_depth\n",
    "    self.invalid_mask_value = -1e3\n",
    "    assert self.start_depth > 1\n",
    "\n",
    "    self.block_num = patch_size * dilation\n",
    "    self.nnum_t = self.nnum_nempty if nempty else self.nnum\n",
    "    self.nnum_a = ((self.nnum_t / self.block_num).ceil() * self.block_num).int()\n",
    "\n",
    "    num = self.max_depth + 1\n",
    "    self.batch_idx = [None] * num\n",
    "    self.patch_mask = [None] * num\n",
    "    self.dilate_mask = [None] * num\n",
    "    self.rel_pos = [None] * num\n",
    "    self.dilate_pos = [None] * num\n",
    "    self.build_t()\n",
    "\n",
    "  def build_t(self):\n",
    "    for d in range(self.start_depth, self.max_depth + 1):\n",
    "      self.build_batch_idx(d)\n",
    "      self.build_attn_mask(d)\n",
    "      self.build_rel_pos(d)\n",
    "\n",
    "  def build_batch_idx(self, depth: int):\n",
    "    batch = self.batch_id(depth, self.nempty)\n",
    "    self.batch_idx[depth] = self.patch_partition(batch, depth, self.batch_size)\n",
    "\n",
    "  def build_attn_mask(self, depth: int):\n",
    "    batch = self.batch_idx[depth]\n",
    "    mask = batch.view(-1, self.patch_size)\n",
    "    self.patch_mask[depth] = self._calc_attn_mask(mask)\n",
    "\n",
    "    mask = batch.view(-1, self.patch_size, self.dilation)\n",
    "    mask = mask.transpose(1, 2).reshape(-1, self.patch_size)\n",
    "    self.dilate_mask[depth] = self._calc_attn_mask(mask)\n",
    "\n",
    "  def _calc_attn_mask(self, mask: torch.Tensor):\n",
    "    attn_mask = mask.unsqueeze(2) - mask.unsqueeze(1)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, self.invalid_mask_value)\n",
    "    return attn_mask\n",
    "\n",
    "  def build_rel_pos(self, depth: int):\n",
    "    key = self.key(depth, self.nempty)\n",
    "    key = self.patch_partition(key, depth)\n",
    "    x, y, z, _ = ocnn.octree.key2xyz(key, depth)\n",
    "    xyz = torch.stack([x, y, z], dim=1)\n",
    "\n",
    "    xyz = xyz.view(-1, self.patch_size, 3)\n",
    "    self.rel_pos[depth] = xyz.unsqueeze(2) - xyz.unsqueeze(1)\n",
    "\n",
    "    xyz = xyz.view(-1, self.patch_size, self.dilation, 3)\n",
    "    xyz = xyz.transpose(1, 2).reshape(-1, self.patch_size, 3)\n",
    "    self.dilate_pos[depth] = xyz.unsqueeze(2) - xyz.unsqueeze(1)\n",
    "\n",
    "  def patch_partition(self, data: torch.Tensor, depth: int, fill_value=0):\n",
    "    num = self.nnum_a[depth] - self.nnum_t[depth]\n",
    "    tail = data.new_full((num,) + data.shape[1:], fill_value)\n",
    "    return torch.cat([data, tail], dim=0)\n",
    "\n",
    "  def patch_reverse(self, data: torch.Tensor, depth: int):\n",
    "    return data[:self.nnum_t[depth]]\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_features: int, hidden_features: Optional[int] = None,\n",
    "               out_features: Optional[int] = None, activation=torch.nn.GELU,\n",
    "               drop: float = 0.0, **kwargs):\n",
    "    super().__init__()\n",
    "    self.in_features = in_features\n",
    "    self.out_features = out_features or in_features\n",
    "    self.hidden_features = hidden_features or in_features\n",
    "\n",
    "    self.fc1 = torch.nn.Linear(self.in_features, self.hidden_features)\n",
    "    self.act = activation()\n",
    "    self.fc2 = torch.nn.Linear(self.hidden_features, self.out_features)\n",
    "    self.drop = torch.nn.Dropout(drop, inplace=True)\n",
    "\n",
    "  def forward(self, data: torch.Tensor):\n",
    "    data = self.fc1(data)\n",
    "    data = self.act(data)\n",
    "    data = self.drop(data)\n",
    "    data = self.fc2(data)\n",
    "    data = self.drop(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class OctreeDWConvBn(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int, kernel_size: List[int] = [3],\n",
    "               stride: int = 1, nempty: bool = False):\n",
    "    super().__init__()\n",
    "    # self.conv = dwconv.OctreeDWConv(\n",
    "    #     in_channels, kernel_size, nempty, use_bias=False)\n",
    "    self.bn = torch.nn.BatchNorm1d(in_channels)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    out = self.conv(data, octree, depth)\n",
    "    out = self.bn(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class RPE(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, patch_size: int, num_heads: int, dilation: int = 1):\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.num_heads = num_heads\n",
    "    self.dilation = dilation\n",
    "    self.pos_bnd = self.get_pos_bnd(patch_size)\n",
    "    self.rpe_num = 2 * self.pos_bnd + 1\n",
    "    self.rpe_table = torch.nn.Parameter(torch.zeros(3*self.rpe_num, num_heads))\n",
    "    torch.nn.init.trunc_normal_(self.rpe_table, std=0.02)\n",
    "\n",
    "  def get_pos_bnd(self, patch_size: int):\n",
    "    return int(0.8 * patch_size * self.dilation**0.5)\n",
    "\n",
    "  def xyz2idx(self, xyz: torch.Tensor):\n",
    "    mul = torch.arange(3, device=xyz.device) * self.rpe_num\n",
    "    xyz = xyz.clamp(-self.pos_bnd, self.pos_bnd)\n",
    "    idx = xyz + (self.pos_bnd + mul)\n",
    "    return idx\n",
    "\n",
    "  def forward(self, xyz):\n",
    "    idx = self.xyz2idx(xyz)\n",
    "    out = self.rpe_table.index_select(0, idx.reshape(-1))\n",
    "    out = out.view(idx.shape + (-1,)).sum(3)\n",
    "    out = out.permute(0, 3, 1, 2)  # (N, K, K, H) -> (N, H, K, K)\n",
    "    return out\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return 'num_heads={}, pos_bnd={}, dilation={}'.format(\n",
    "            self.num_heads, self.pos_bnd, self.dilation)  # noqa\n",
    "\n",
    "\n",
    "class OctreeAttention(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, dim: int, patch_size: int, num_heads: int,\n",
    "               qkv_bias: bool = True, qk_scale: Optional[float] = None,\n",
    "               attn_drop: float = 0.0, proj_drop: float = 0.0,\n",
    "               dilation: int = 1, use_rpe: bool = True):\n",
    "    super().__init__()\n",
    "    self.dim = dim\n",
    "    self.patch_size = patch_size\n",
    "    self.num_heads = num_heads\n",
    "    self.dilation = dilation\n",
    "    self.use_rpe = use_rpe\n",
    "    self.scale = qk_scale or (dim // num_heads) ** -0.5\n",
    "\n",
    "    self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "    self.attn_drop = torch.nn.Dropout(attn_drop)\n",
    "    self.proj = torch.nn.Linear(dim, dim)\n",
    "    self.proj_drop = torch.nn.Dropout(proj_drop)\n",
    "    self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    # NOTE: self.rpe is not used in the original experiments of my paper. When\n",
    "    # releasing the code, I added self.rpe because I observed that it could\n",
    "    # stablize the training process and improve the performance on ScanNet by\n",
    "    # 0.3 to 0.5; on the other datasets, the improvements are more marginal. So\n",
    "    # it is not indispensible, and can be removed by setting `use_rpe` as False.\n",
    "    self.rpe = RPE(patch_size, num_heads, dilation) if use_rpe else None\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: OctreeT, depth: int):\n",
    "    H = self.num_heads\n",
    "    K = self.patch_size\n",
    "    C = self.dim\n",
    "    D = self.dilation\n",
    "\n",
    "    # patch partition\n",
    "    data = octree.patch_partition(data, depth)\n",
    "    if D > 1:  # dilation\n",
    "      rel_pos = octree.dilate_pos[depth]\n",
    "      mask = octree.dilate_mask[depth]\n",
    "      data = data.view(-1, K, D, C).transpose(1, 2).reshape(-1, C)\n",
    "    else:\n",
    "      rel_pos = octree.rel_pos[depth]\n",
    "      mask = octree.patch_mask[depth]\n",
    "    data = data.view(-1, K, C)\n",
    "\n",
    "    # qkv\n",
    "    qkv = self.qkv(data).reshape(-1, K, 3, H, C // H).permute(2, 0, 3, 1, 4)\n",
    "    q, k, v = qkv[0], qkv[1], qkv[2]      # (N, H, K, C')\n",
    "    q = q * self.scale\n",
    "\n",
    "    # attn\n",
    "    attn = q @ k.transpose(-2, -1)        # (N, H, K, K)\n",
    "    attn = self.apply_rpe(attn, rel_pos)  # (N, H, K, K)\n",
    "    attn = attn + mask.unsqueeze(1)\n",
    "    attn = self.softmax(attn)\n",
    "    attn = self.attn_drop(attn)\n",
    "    data = (attn @ v).transpose(1, 2).reshape(-1, C)\n",
    "\n",
    "    # patch reverse\n",
    "    if D > 1:  # dilation\n",
    "      data = data.view(-1, D, K, C).transpose(1, 2).reshape(-1, C)\n",
    "    data = octree.patch_reverse(data, depth)\n",
    "\n",
    "    # ffn\n",
    "    data = self.proj(data)\n",
    "    data = self.proj_drop(data)\n",
    "    return data\n",
    "\n",
    "  def apply_rpe(self, attn, rel_pos):\n",
    "    if self.use_rpe:\n",
    "      attn = attn + self.rpe(rel_pos)\n",
    "    return attn\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return 'dim={}, patch_size={}, num_heads={}, dilation={}'.format(\n",
    "            self.dim, self.patch_size, self.num_heads, self.dilation)  # noqa\n",
    "\n",
    "\n",
    "class OctFormerBlock(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, dim: int, num_heads: int, patch_size: int = 32,\n",
    "               dilation: int = 0, mlp_ratio: float = 4.0, qkv_bias: bool = True,\n",
    "               qk_scale: Optional[float] = None, attn_drop: float = 0.0,\n",
    "               proj_drop: float = 0.0, drop_path: float = 0.0, nempty: bool = True,\n",
    "               activation: torch.nn.Module = torch.nn.GELU, **kwargs):\n",
    "    super().__init__()\n",
    "    self.norm1 = torch.nn.LayerNorm(dim)\n",
    "    self.attention = OctreeAttention(dim, patch_size, num_heads, qkv_bias,\n",
    "                                     qk_scale, attn_drop, proj_drop, dilation)\n",
    "    self.norm2 = torch.nn.LayerNorm(dim)\n",
    "    self.mlp = MLP(dim, int(dim * mlp_ratio), dim, activation, proj_drop)\n",
    "    self.drop_path = ocnn.nn.OctreeDropPath(drop_path, nempty)\n",
    "    self.cpe = OctreeDWConvBn(dim, nempty=nempty)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: OctreeT, depth: int):\n",
    "    data = self.cpe(data, octree, depth) + data\n",
    "    attn = self.attention(self.norm1(data), octree, depth)\n",
    "    data = data + self.drop_path(attn, octree, depth)\n",
    "    ffn = self.mlp(self.norm2(data))\n",
    "    data = data + self.drop_path(ffn, octree, depth)\n",
    "    return data\n",
    "\n",
    "\n",
    "class OctFormerStage(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, dim: int, num_heads: int, patch_size: int = 32,\n",
    "               dilation: int = 0, mlp_ratio: float = 4.0, qkv_bias: bool = True,\n",
    "               qk_scale: Optional[float] = None, attn_drop: float = 0.0,\n",
    "               proj_drop: float = 0.0, drop_path: float = 0.0, nempty: bool = True,\n",
    "               activation: torch.nn.Module = torch.nn.GELU, interval: int = 6,\n",
    "               use_checkpoint: bool = True, num_blocks: int = 2,\n",
    "               octformer_block=OctFormerBlock, **kwargs):\n",
    "    super().__init__()\n",
    "    self.num_blocks = num_blocks\n",
    "    self.use_checkpoint = use_checkpoint\n",
    "    self.interval = interval  # normalization interval\n",
    "    self.num_norms = (num_blocks - 1) // self.interval\n",
    "\n",
    "    self.blocks = torch.nn.ModuleList([octformer_block(\n",
    "        dim=dim, num_heads=num_heads, patch_size=patch_size,\n",
    "        dilation=1 if (i % 2 == 0) else dilation,\n",
    "        mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "        attn_drop=attn_drop, proj_drop=proj_drop,\n",
    "        drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "        nempty=nempty, activation=activation) for i in range(num_blocks)])\n",
    "    # self.norms = torch.nn.ModuleList([\n",
    "    #     torch.nn.BatchNorm1d(dim) for _ in range(self.num_norms)])\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: OctreeT, depth: int):\n",
    "    for i in range(self.num_blocks):\n",
    "      if self.use_checkpoint and self.training:\n",
    "        data = checkpoint(self.blocks[i], data, octree, depth)\n",
    "      else:\n",
    "        data = self.blocks[i](data, octree, depth)\n",
    "      # if i % self.interval == 0 and i != 0:\n",
    "      #   data = self.norms[(i - 1) // self.interval](data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class PatchEmbed(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int = 3, dim: int = 96, num_down: int = 2,\n",
    "               nempty: bool = True, **kwargs):\n",
    "    super().__init__()\n",
    "    self.num_stages = num_down\n",
    "    self.delta_depth = -num_down\n",
    "    channels = [int(dim * 2**i) for i in range(-self.num_stages, 1)]\n",
    "\n",
    "    self.convs = torch.nn.ModuleList([ocnn.modules.OctreeConvBnRelu(\n",
    "        in_channels if i == 0 else channels[i], channels[i], kernel_size=[3],\n",
    "        stride=1, nempty=nempty) for i in range(self.num_stages)])\n",
    "    self.downsamples = torch.nn.ModuleList([ocnn.modules.OctreeConvBnRelu(\n",
    "        channels[i], channels[i+1], kernel_size=[2], stride=2, nempty=nempty)\n",
    "        for i in range(self.num_stages)])\n",
    "    self.proj = ocnn.modules.OctreeConvBnRelu(\n",
    "        channels[-1], dim, kernel_size=[3], stride=1, nempty=nempty)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    for i in range(self.num_stages):\n",
    "      depth_i = depth - i\n",
    "      data = self.convs[i](data, octree, depth_i)\n",
    "      data = self.downsamples[i](data, octree, depth_i)\n",
    "    data = self.proj(data, octree, depth_i - 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "class Downsample(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int, out_channels: int,\n",
    "               kernel_size: List[int] = [2], nempty: bool = True):\n",
    "    super().__init__()\n",
    "    self.norm = torch.nn.BatchNorm1d(out_channels)\n",
    "    self.conv = ocnn.nn.OctreeConv(in_channels, out_channels, kernel_size,\n",
    "                                   stride=2, nempty=nempty, use_bias=True)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    data = self.conv(data, octree, depth)\n",
    "    data = self.norm(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "class OctFormer(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int,\n",
    "               channels: List[int] = [96, 192, 384, 384],\n",
    "               num_blocks: List[int] = [2, 2, 18, 2],\n",
    "               num_heads: List[int] = [6, 12, 24, 24],\n",
    "               patch_size: int = 26, dilation: int = 4, drop_path: float = 0.5,\n",
    "               nempty: bool = True, stem_down: int = 2, **kwargs):\n",
    "    super().__init__()\n",
    "    self.patch_size = patch_size\n",
    "    self.dilation = dilation\n",
    "    self.nempty = nempty\n",
    "    self.num_stages = len(num_blocks)\n",
    "    self.stem_down = stem_down\n",
    "    drop_ratio = torch.linspace(0, drop_path, sum(num_blocks)).tolist()\n",
    "\n",
    "    self.patch_embed = PatchEmbed(in_channels, channels[0], stem_down, nempty)\n",
    "    self.layers = torch.nn.ModuleList([OctFormerStage(\n",
    "        dim=channels[i], num_heads=num_heads[i], patch_size=patch_size,\n",
    "        drop_path=drop_ratio[sum(num_blocks[:i]):sum(num_blocks[:i+1])],\n",
    "        dilation=dilation, nempty=nempty, num_blocks=num_blocks[i],)\n",
    "        for i in range(self.num_stages)])\n",
    "    self.downsamples = torch.nn.ModuleList([Downsample(\n",
    "        channels[i], channels[i + 1], kernel_size=[2],\n",
    "        nempty=nempty) for i in range(self.num_stages - 1)])\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    data = self.patch_embed(data, octree, depth)\n",
    "    depth = depth - self.stem_down   # current octree depth\n",
    "    octree = OctreeT(octree, self.patch_size, self.dilation, self.nempty,\n",
    "                     max_depth=depth, start_depth=depth-self.num_stages+1)\n",
    "    features = {}\n",
    "    for i in range(self.num_stages):\n",
    "      depth_i = depth - i\n",
    "      data = self.layers[i](data, octree, depth_i)\n",
    "      features[depth_i] = data\n",
    "      if i < self.num_stages - 1:\n",
    "        data = self.downsamples[i](data, octree, depth_i)\n",
    "    return features\n",
    "\n",
    "class ClsHeader(torch.nn.Module):\n",
    "  def __init__(self, out_channels: int, in_channels: int,\n",
    "               nempty: bool = False, dropout: float = 0.5):\n",
    "    super().__init__()\n",
    "    self.global_pool = ocnn.nn.OctreeGlobalPool(nempty)\n",
    "    self.cls_header = torch.nn.Sequential(\n",
    "        ocnn.modules.FcBnRelu(in_channels, 256),\n",
    "        torch.nn.Dropout(p=dropout),\n",
    "        torch.nn.Linear(256, out_channels))\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    data = self.global_pool(data, octree, depth)\n",
    "    logit = self.cls_header(data)\n",
    "    return logit\n",
    "\n",
    "\n",
    "class OctFormerCls(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, in_channels: int, out_channels: int,\n",
    "               channels: List[int] = [96, 192, 384, 384],\n",
    "               num_blocks: List[int] = [2, 2, 18, 2],\n",
    "               num_heads: List[int] = [6, 12, 24, 24],\n",
    "               patch_size: int = 32, dilation: int = 4,\n",
    "               drop_path: float = 0.5, nempty: bool = True,\n",
    "               stem_down: int = 2, head_drop: float = 0.5, **kwargs):\n",
    "    super().__init__()\n",
    "    self.backbone = OctFormer(\n",
    "        in_channels, channels, num_blocks, num_heads, patch_size, dilation,\n",
    "        drop_path, nempty, stem_down)\n",
    "    self.head = ClsHeader(\n",
    "        out_channels, channels[-1], nempty, head_drop)\n",
    "    self.apply(self.init_weights)\n",
    "\n",
    "  def init_weights(self, m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "      torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "      if isinstance(m, torch.nn.Linear) and m.bias is not None:\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "  def forward(self, data: torch.Tensor, octree: Octree, depth: int):\n",
    "    features = self.backbone(data, octree, depth)\n",
    "    curr_depth = min(features.keys())\n",
    "    output = self.head(features[curr_depth], octree, curr_depth)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUnitSurf(surf, mean_arr = None, scale_factor = None, copy=True):\n",
    "    unit_surf, surf_mean, surf_scale = ScaleSurf(surf, mean_arr, scale_factor, copy)\n",
    "    return unit_surf\n",
    "\n",
    "def GetNormalsTensor(surf):\n",
    "    normals = ComputeNormals(surf)\n",
    "    normals = vtk_to_numpy(normals.GetPointData().GetNormals())\n",
    "    return torch.tensor(normals, dtype=torch.double)\n",
    "\n",
    "def ComputeNormals(surf):\n",
    "    normals = vtk.vtkPolyDataNormals()\n",
    "    normals.SetInputData(surf);\n",
    "    normals.ComputeCellNormalsOn();\n",
    "    normals.ComputePointNormalsOn();\n",
    "    normals.SplittingOff();\n",
    "    normals.Update()\n",
    "    \n",
    "    return normals.GetOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The shape of input data is wrong.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39meval() \n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moctree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moctree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 415\u001b[0m, in \u001b[0;36mOctFormerCls.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, octree: Octree, depth: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 415\u001b[0m   features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m   curr_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(features\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    417\u001b[0m   output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(features[curr_depth], octree, curr_depth)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 362\u001b[0m, in \u001b[0;36mOctFormer.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, octree: Octree, depth: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 362\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m   depth \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem_down   \u001b[38;5;66;03m# current octree depth\u001b[39;00m\n\u001b[1;32m    364\u001b[0m   octree \u001b[38;5;241m=\u001b[39m OctreeT(octree, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnempty,\n\u001b[1;32m    365\u001b[0m                    max_depth\u001b[38;5;241m=\u001b[39mdepth, start_depth\u001b[38;5;241m=\u001b[39mdepth\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stages\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 314\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_stages):\n\u001b[1;32m    313\u001b[0m   depth_i \u001b[38;5;241m=\u001b[39m depth \u001b[38;5;241m-\u001b[39m i\n\u001b[0;32m--> 314\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamples[i](data, octree, depth_i)\n\u001b[1;32m    316\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(data, octree, depth_i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/modules/modules.py:71\u001b[0m, in \u001b[0;36mOctreeConvBnRelu.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: torch\u001b[38;5;241m.\u001b[39mTensor, octree: Octree, depth: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     69\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m''''''\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(out)\n\u001b[1;32m     73\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/nn/octree_conv.py:357\u001b[0m, in \u001b[0;36mOctreeConv.forward\u001b[0;34m(self, data, octree, depth)\u001b[0m\n\u001b[1;32m    355\u001b[0m   out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(col\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43moctree_conv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moctree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnempty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    363\u001b[0m   out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/nn/octree_conv.py:224\u001b[0m, in \u001b[0;36mOctreeConvFunction.forward\u001b[0;34m(ctx, data, weights, octree, depth, in_channels, out_channels, kernel_size, stride, nempty, max_buffer)\u001b[0m\n\u001b[1;32m    221\u001b[0m octree_conv \u001b[38;5;241m=\u001b[39m _OctreeConv(\n\u001b[1;32m    222\u001b[0m     in_channels, out_channels, kernel_size, stride, nempty, max_buffer)\n\u001b[1;32m    223\u001b[0m octree_conv\u001b[38;5;241m.\u001b[39msetup(octree, depth)\n\u001b[0;32m--> 224\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43moctree_conv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m out \u001b[38;5;241m=\u001b[39m octree_conv\u001b[38;5;241m.\u001b[39mforward_gemm(out, data, weights)\n\u001b[1;32m    227\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(data, weights)\n",
      "File \u001b[0;32m/work/floda/source/tools/miniconda3/envs/flo_env/lib/python3.9/site-packages/ocnn/nn/octree_conv.py:101\u001b[0m, in \u001b[0;36mOctreeConvBase.check_and_init\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Check the shape of input data\u001b[39;00m\n\u001b[1;32m    100\u001b[0m check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_shape\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m check, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe shape of input data is wrong.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Init the output data\u001b[39;00m\n\u001b[1;32m    104\u001b[0m out \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnew_zeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_shape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The shape of input data is wrong."
     ]
    }
   ],
   "source": [
    "from ocnn.dataset import CollateBatch\n",
    "from thsolver import Dataset\n",
    "\n",
    "# channel: 4\n",
    "# feature: ND\n",
    "# find_unused_parameters: False\n",
    "# name: octformercls\n",
    "# nempty: False\n",
    "# nout: 40\n",
    "# sync_bn: False\n",
    "# use_checkpoint: False\n",
    "\n",
    "path = '/work/floda/source/tools/octformer/data/ModelNet40/ModelNet40/airplane/train/airplane_0001.off'\n",
    "\n",
    "surf1 = ReadSurf(path)\n",
    "surf1 = GetUnitSurf(surf1)\n",
    "V1, F1 = PolyDataToTensors_v_f(surf1)\n",
    "N1 = GetNormalsTensor(surf1)\n",
    "\n",
    "octree = Octree(6)\n",
    "octree.build_octree(Points(V1, normals=N1))\n",
    "octree.construct_all_neigh()\n",
    "octree_feature = ocnn.modules.InputFeature('ND')\n",
    "data = octree_feature(octree)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "in_channels = 4 \n",
    "out_channels = 1\n",
    "\n",
    "model = OctFormerCls(in_channels=in_channels, out_channels=out_channels)\n",
    "model = model.to(device)\n",
    "\n",
    "label = torch.tensor([0]).to(device) \n",
    "fake_batch = {\n",
    "    'octree': octree.to(device),\n",
    "    'data': data.to(device),\n",
    "    'label': label\n",
    "}\n",
    "model.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(fake_batch['data'], fake_batch['octree'], fake_batch['octree'].depth)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
